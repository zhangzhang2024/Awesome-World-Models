# Awesome-World-Models-for-Embodied-AI [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
This repo collects benchmark, papers, and codes about embodied world models.

If you find this repository useful, please consider  **giving us a star** ðŸŒŸ

## Papers
### 2025
- _**Occupancy World Model for Robots**_ <br>
**`arXiv25.05`** [[Paper]](https://arxiv.org/pdf/2505.05512v1.pdf) <br>

- _**Learning 3D Persistent Embodied World Models**_ <br>
**`arXiv25.05`** [[Paper]](https://arxiv.org/pdf/2505.05495.pdf) <br>

- _**TesserAct: Learning 4D Embodied World Models**_ <br>
**`arXiv25.04`** [[Paper]](https://arxiv.org/pdf/2504.20995v1.pdf) [[Code]](https://github.com/UMass-Embodied-AGI/TesserAct) <br>

- _**Navigation World Models**_ <br>
**`CVPR 2025`** [[Paper]](https://arxiv.org/pdf/2412.03572.pdf) [[Code]](https://github.com/facebookresearch/nwm/) <br>

- _**COMBO: Compositional World Models for Embodied Multi-Agent Cooperation**_ <br>
**`ICLR 2025`** [[Paper]](https://arxiv.org/pdf/2404.10775.pdf) [[Code]](https://github.com/UMass-Embodied-AGI/COMBO) <br>

- _**Cosmos World Foundation Model Platform for Physical AI**_ <br>
**`arXiv25.03`** [[Paper]](https://arxiv.org/pdf/2501.03575.pdf) <br>

- _**EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation**_ <br>
**`arXiv25.02`** [[Paper]](https://arxiv.org/pdf/2501.01895.pdf) <br>

- _**NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants**_ <br>
**`arXiv25.02`** [[Paper]](https://arxiv.org/pdf/2502.13894.pdf) <br>

- _**GenEx: Generating an Explorable World**_ <br>
**`arXiv25.01`** [[Paper]](https://arxiv.org/pdf/2412.09624.pdf) <br>

### 2024
- _**WHALE: Towards Generalizable and Scalable World Models for Embodied Decision-making**_ <br>
**`arXiv24.11`** [[Paper]](https://arxiv.org/pdf/2411.05619.pdf) <br>

- _**MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control**_ <br>
**`NIPS 2024 @ OWA`** [[Paper]](https://arxiv.org/pdf/2403.12037.pdf) [[Code]](https://github.com/Zhoues/MineDreamer) <br>

- _**UniSim: Learning Interactive Real-World Simulators**_ <br>
**`ICLR 2024`** [[Paper]](https://arxiv.org/pdf/2310.06114.pdf) <br>

- _**Video Language Planning**_ <br>
**`ICLR 2024`** [[Paper]](https://arxiv.org/pdf/2310.10625.pdf) [[Code]](https://github.com/video-language-planning/vlp_code) <br>

- _**IRASim: Learning Interactive Real-Robot Action Simulators**_ <br>
**`arXiv24.06`** [[Paper]](https://arxiv.org/pdf/2406.14540.pdf) [[Code]](https://github.com/bytedance/IRASim) <br>

- _**RoboDreamer: Learning Compositional World Models for Robot Imagination**_ <br>
**`arXiv24.04`** [[Paper]](https://arxiv.org/pdf/2404.12377.pdf) <br>

### 2023
- _**Learning Universal Policies via Text-Guided Video Generation**_ <br>
**`NIPS 2023`** [[Paper]](https://arxiv.org/pdf/2302.00111.pdf) <br>

- _**DreamerV3: Mastering Diverse Domains through World Models**_ <br>
**`arXiv23.01`** [[Paper]](https://arxiv.org/pdf/2301.04104.pdf) [[Code]](https://github.com/danijar/dreamerv3) <br>
